# デーレイクのコンポーネント

## データレイクのアーキテクチャー

データレイクのアーキテクチャーは以下の図のようになる。

![データレイクのアーキテクチャー](./images/データレイク.png)

## ラムダアーキテクチャー

バッチ処理とストリーム処理の両方を組み合わせたラムダアーキテクチャーがある。  
バッチ処理は大量データを並列で高速に処理できるが最新のデータを処理できない。  
ストリーム処理は最新のデータを処理できるが、複雑になる。よって再集計する場合手間がかかる。  
最新のデータはストリーム処理から取得し、バッチ処理側のデータとマージすることで最新の集計が可能。

![ラムダアーキテクチャー](./images/ラムダアーキテクチャー.png)

## データレイクを構築する際の考え方

1. どのようなアウトプットがほしいかを考える
   - 経営者が KPI を確認するためのダッシュボード（BI ツール）
   - 不正な決済処理のリアルタイム検知（ストリーム分析）
   - ログ分析（分散 SQL）
2. 必要なデータソースのリストアップ
3. データの収集方法
   - リアルタイム性が欲しい場合はストリーム処理、そうでない場合はバッチ処理
   - RDB でストリーム処理をする場合は変更データキャプチャーを使い、バッチ処理する場合はレプリケーションログを使用するなど
4. アウトプットを導出するための変換を決める
   - Glue Job や Spark を用いて変換する

まず「分析」（分析から得られるアウトプット）を考え、それを得るためのデータを「収集」「保存」し、最後にデータをアウトプットするための「変換」「分析」について考える。

## 各コンポーネントの AWS サービス

![コンポーネントAWSサービス対応関係](./images/コンポーネントAWSサービス対応関係.png)

## 収集コンポーネント

S3 には API があるので子の API を用いて、オンプレミスのサーバーやファイルシステムなどにあるデータを転送する。

### ファイル

業務システムなどから日時で出力される CSV ファイルを、cron などを用いてデータレイクに保存する。  
AWS のサービスでは AWS DataSync がある。データ転送用のエージェントを用いてオンプレミスにあるデータを S3 に対して自動的に転送できる。

### ストリームデータ

ストリームデータとは、継続的にデータが発生して順に流れてくるものを指す。Web サービスのイベント履歴、IoT センサーから定期的に送られてくるメトリクス、Web サーバーにアクセスした際に生成されるアクセスログなどが典型的なストリームデータ。ストリームデータは小さめ（数百 KB 未満）かつ大量に生成される。  
AWS のサービスでは、Amazon KDS（Kinesis Data Streams）がある。Kinesis Data Streams は log-structured な形式で書き込まれるため順序整合性があり、パーティショニングされるためスケーリングでき、レプリケーションされるため耐久性があるストリーミングサービス。  
Amazon KDF（Kinesis Data Firehose）は、S3/Redshift/ElasticSearchService などに圧縮形式やデータフォーマットを指定し簡単な変換をしてデータを出力できるサービス。  
Amazon MSK（Managed Streaming for Kafka）は、Kafka のマネージドサービス。

### データベース

RDB からデータを収集する場合は、ストリーミングによる収集とバッチによる収集の 2 種類がある。ストリーミングによる収集はレプリケーションログを使用した変更データキャプチャーによって収集する。バッチによる収集はレプリケーションログを利用した S3 への差分抽出がある。  
AWS のサービスの場合は、ストリーミング/バッチともに AWS DMS（Database Migration Service）を利用できる。

## 保存コンポーネント

### ストレージ

AWS S3 を使用する。

ストレージに求められる要件

1. 耐久性
   - 3 つ以上の AZ にレプリケーションする
2. 可用性
3. スケーラビリティー
4. 多様な形式のデータを保存可能
5. コスト
   - 参照頻度が低いまたは古いデータをアーカイブしてコスト削減できる
6. セキュリティー/権限管理
   - HTTPS で通信経路の暗号化・KMS でストレージの暗号化が可能
   - BucketPolicy/IAM/アクセスコントロールリストで権限管理可能

### メタデータ

AWS Glue Data Catalog を使用する。S3 上のパスやファイルの中身のデータのスキーマを保存できる。S3 だけでなく RDB・Redshift などのスキーマも取得可能。  
Glue Data Catalog は、Apache Hive Metastore と互換性があるので、Hadoop エコシステムの Spark・Trino などが Glue Data Catalog のスキーマを読み取って使用することが可能。AWS のサービスでは Redshift Spectrum を Glue Data Catalog のスキーマを外部スキーマとして取り込むことで S3 にクエリできる。Athena は、Trino・Apache Spark フレームワーク上に構築されているサーバーレスの分散 SQL なので Glue Data Catalog のスキーマを利用して S3 に SQL をリクエストできる。

## 変換コンポーネント

変換コンポーネントでは以下のようなデータ変換を行う。

- 不正な値の処理: null やから文字列の変換など
- 文字列や値の統一: 大文字小文字の統一・表記の統一・整数/小数の統一
- テーブルの結合処理
- ファイルサイズの変換: 大量の数 KB のファイルを数百 MB のファイルに集約する
- ファイルフォーマットの変換: Parquet への変換

データレイクでは、生のデータ置き場と変換後のデータ置き場を明示的に分離する。分離することで真のデータ置き場が分かりやすくなり、権限管理もしやすくなる。

### Apache Parquet

Parquet は、列指向のファイルフォーマット。OLAP（Online Analytical Processing）の場合、大量のデータを一部のカラムを使用して集計することが多いため、必要な列だけを読み込める列指向のファイルが使用される。列指向のファイルはデータを更新する場合、バラバラに格納されたすべての列を更新する必要があるため write once, read many のユースケースに向いている。

![parquet](./images/parquet.gif)

### データ変換に求められること

1. データ活用のための最適化
   - 分析時のビジネス的な最適化: フォーマットの統一・不正な値の処理など
   - 分析パフォーマンスの最適化: フォーマット・ファイルサイズの変換・パーティションなど
2. データ変換の運用容易性
   - スケーラビリティー: 複数台のノードで変換処理をする
   - 冪等性: ある操作を何度繰り返しても同じ操作になるようにする。（分散処理中 1 つのノードがクラッシュしたり、一部のデータが遅れてストレージに格納された場合にもう一度最初から変換処理を行わなければならない）
   - 保守性: バージョン管理

### AWS のサービス

AWS Glue Job を使用する。実行エンジンとして Apache Spark・Python を使用できる。Spark は分散処理できるので主に使用し、単純な処理は Python を使用する。  
Glue Workflow を使用すると Spark または Python スクリプトの処理を繋げたものをワークフローとして管理できる。

## 分析コンポーネント

分析を行う手法は大別すると 2 種類ある。ストレージに蓄積されたデータに対する分析と、ストリームデータに対する分析。

### 蓄積されたデータに対する分析

1. BI ツール
2. 分散 SQL
3. データウェアハウス
4. データ加工
5. 機械学習

#### BI

BI ツールではレポーティング・分析の用途に使用する。  
レポーティングでは導出されたデータをダッシュボードにグラフとして表示する。

![BIツールレポート](./images/BIツールレポート.png)

分析では DB に対して集計されていないデータに対してクエリを投げて分析する。

![BIツール分析](./images/BIツール分析.png)

AWS のサービスでは QuickSight がある。QuickSight はインメモリ DB である Spice を持ち、S3/Redshift/RDB など様々なデータソースに対応している。

#### 分散 SQL

分散 SQL は、様々な形式のデータに対して SQL をクエリを投げることができる。分散 SQL は以下の場合に使用する。

1. データが半構造化・非構造化データ
2. ワークロードに波がある
3. 処理内容がシンプル

AWS のサービスでは Athena がある。Athena は S3 上のデータに対して直接クエリを投入できるサービス。Glue DataCatalog をスキーマとして使用して、Trino・Spark を処理エンジンとして S3 をストレージとして SQL クエリを投げることができる。

![Athena](./images/Athena.png)

#### データウェアハウス

データウェアハウスは OLAP に特化したデータベース。

|                | RDB                                        | データウェアハウス                                   |
| :------------- | :----------------------------------------- | :--------------------------------------------------- |
| 読み込み       | 1 度のリクエストで少数のレコードを読み込む | 1 度のリクエストで大量のレコードをスキャンする       |
| 書き込み       | 1 度のリクエストで少数のレコードに書き込む | 1 度のリクエストで大量のレコードを書き込む           |
| データの不変性 | データは mutable                           | データが immutable（列指向なので更新に時間がかかる） |
| ストレージ     | 行指向                                     | 列指向                                               |

データウェアハウスでは大量のレコードをスキャンすることができるが、定型的な集計に関しては別途データマートに切り出して負荷を下げる運用を行う。

![データマート](./images/データマート.png)

AWS のサービスでは Redshift がある。大量のクエリが投げられた際はバックグラウンドで別のクラスターを起動して並列で処理する。

1. リーダーノード
   - クエリのエンドポイント
   - クエリオプティマイザが SQL 処理コードを生成しコンピュートノードに展開
2. コンピュートノード
   - クエリの並列実行エンジン
   - 高速ローカル SSD + 大容量 RAM + 広帯域ネットワーク
3. マネージドストレージ
   - Redshift 管理 S3 バケット
   - データの永続ストレージ

![Redshift](./images/Redshift.png)

Redshift Spectrum は、S3 上のデータに対してクエリを投げることができる。外部スキーマとして Glue DataCatalog を使用する。

![Redshift Spectrum](./images/RedshiftSpectrum.png)

### ストリームデータに対する分析

ストリームデータに対する分析方法は主に 2 つある。

1. リアルタイムダッシュボード
   - 流れてくるストリームデータを DB に格納し、すぐにダッシュボードに反映し可視化する
   - 主に GUI ベースでの分析に使用する。CUI を使用して DB に直接クエリを投げることも可能
2. ストリーム分析アプリケーション
   - ストリームデータに対して直接分析を行う
   - クレジットカード使用の異常検知などに使用される

#### リアルタイムダッシュボード

BI のユースケースに似ている。リアルタイムダッシュボードは BI よりもリアルタイム性を重視している。  
ElasticSearch と Kibana を使用して構築可能。

![リアルタイムダッシュボード](./images/リアルタイムダッシュボード.png)

#### ストリーム分析アプリケーション

ストリームで流れてくる 1 件 1 件のレコード または これらを複数まとめたものに対して直接様々な処理を行う分析。典型的なユースケースとして EC サイトでログインリクエストを集計して不正ログインの疑いがあるリクエストを抽出し、1 分ごとの不正ログインリクエストが一定の閾値を超えたらアラートを上げるといったものがある。こうした処理を行う場合、ストリームデータを加工し、不正スコアを算出し、最後にスコアが一定以上のレコードを一分ごとに集計するといった 3 つの処理を組み合わせる。

- 加工整形  
   一件ずつや数十件の少量データを対象として短いサイクルで何度も変換を行う。次のプロセスにできるだけ早くストリームを流すために短いサイクルで変換を実行する。
- 集計  
  一定の期間内に溜まったデータを対象として、平均値・合計値・最大値・最小値などの集計を行う。主計を行うためには、一定期間内にやってくるデータをすべてメモリに保持する必要がある。そのため集計対象の時間幅は短くなる。数十秒から数十分の範囲。
- 異常検知・予測  
  ストリームデータに対する予測や異常検知を行う。

AWS のサービスでは、Amazon KDA（Kinesis Data Analytics）がある。KDS（Kinesis Data Streams）や（Kinesis Data Firehose）で取得したデータに対して SQL ベースでの分析が可能。分析結果を KDS や KDF に書き戻したり、AWS Lambda に引き渡したりもできる。OSS としては Spark Streaming などがある。

![ストリーム分析アプリケーション](./images/ストリーム分析.png)
